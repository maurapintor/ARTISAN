{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Gradient Obfuscation and Adaptive Attacks\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/maurapintor/ARTISAN/blob/solutions/04_gradient_obfuscation_and_adaptive_attacks.ipynb)\n",
    "\n",
    "In this tutorial, we are going to test the robustness of two defenses that were specifically designed to be strong against gradient-based attacks.\n",
    "\n",
    "Despite the efforts, however, these models were later shown to be just *hiding* the adversarial examples rather than removing them. They were broken by **adaptive** attacks, *i.e.*, attacks that target the specific defense mechanism that is in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import secml\n",
    "except ImportError:\n",
    "    %pip install secml\n",
    "\n",
    "try:\n",
    "    import foolbox\n",
    "except ImportError:\n",
    "    %pip install foolbox\n",
    "\n",
    "try:\n",
    "    import robustbench\n",
    "except ImportError:\n",
    "    %pip install git+https://github.com/RobustBench/robustbench.git\n",
    "\n",
    "import os\n",
    "if not os.path.exists(\"models\"):\n",
    "    !git clone https://github.com/maurapintor/ARTISAN.git\n",
    "    os.chdir(\"ARTISAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## The k-Winners-Take-All (k-WTA) Defense\n",
    "\n",
    "This defense was proposed in [1]. This defense replaces the usual activation layer of the DNNs (ReLU) with a k-WTA activation function.\n",
    "\n",
    "As shown in the picture, the ReLU sets to zero the neurons that have a negative output, whereas the k-WTA sets to zero all weights except the top-k (optionally in absolute value, there are different possible configurations).\n",
    "\n",
    "![](assets/kWTA.png)\n",
    "\n",
    "This trick aims to break gradient descent by introducing $C_0$ discontinuities on the loss landscape. In this way, any gradient-based attack would fail in finding a consistent direction to minimize the loss.\n",
    "\n",
    "[1] Xiao, Chang, Peilin Zhong, and Changxi Zheng. \"Enhancing Adversarial Defense by k-Winners-Take-All.\" International Conference on Learning Representations. 2019.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from robustbench.utils import download_gdrive\n",
    "from secml.array import CArray\n",
    "from secml.ml import CClassifierPyTorch\n",
    "from models.kwta import SparseResNet18\n",
    "\n",
    "if not os.path.exists(\"pretrained\"):\n",
    "    os.mkdir(\"pretrained\")\n",
    "\n",
    "# key of a Google Drive file containing the pretrained model\n",
    "MODEL_ID = '1Af_owmMvg1LxjITLE1gFUmPx5idogeTP'\n",
    "\n",
    "gamma = 0.1  # sparsity ratio\n",
    "filepath = os.path.join('pretrained', f'kwta_spresnet18_{gamma}_cifar_adv.pth')\n",
    "if not os.path.exists(filepath):\n",
    "    # utility function to handle google drive data\n",
    "    download_gdrive(MODEL_ID, filepath)\n",
    "\n",
    "# check out the model class in the imported module to see the implementation\n",
    "# of this defense\n",
    "model = SparseResNet18(sparsities=[gamma, gamma, gamma, gamma])\n",
    "\n",
    "# check if CUDA is available\n",
    "if not torch.cuda.is_available():\n",
    "    state_dict = torch.load(filepath, map_location='cpu')\n",
    "else:\n",
    "    state_dict = torch.load(filepath)\n",
    "\n",
    "# load model and wrap it in secml classifier class\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "clf = CClassifierPyTorch(model, input_shape=(3, 32, 32), pretrained=True,\n",
    "                         pretrained_classes=CArray(list(range(10))), preprocess=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load CIFAR10 dataset (we only need the test set)\n",
    "from secml.data.loader import CDataLoaderCIFAR10\n",
    "_, ts = CDataLoaderCIFAR10().load()\n",
    "\n",
    "# let's use a subset of the data\n",
    "ts = ts[:100, :]\n",
    "\n",
    "# transform data to stay in [0, 1] bounds\n",
    "ts.X /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# check model accuracy on the CIFAR10 data\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "\n",
    "metric = CMetricAccuracy()\n",
    "preds = clf.predict(ts.X)\n",
    "\n",
    "acc = metric.performance_score(y_true=ts.Y, y_pred=preds)\n",
    "print(f\"Model accuracy: {acc*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise\n",
    "Attack the model with untargeted PGD $\\ell_\\infty$, $\\varepsilon=0.03$, $\\alpha=0.005$ and 30 steps.\n",
    "Apply it to the first 10 samples of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = 10\n",
    "samples, labels = ts.X[:idx, :], ts.Y[:idx]\n",
    "y_pred = clf.predict(samples)\n",
    "\n",
    "from secml.adv.attacks import CFoolboxPGDLinf\n",
    "\n",
    "epsilon = 0.03\n",
    "abs_stepsize = 0.005\n",
    "steps = 30\n",
    "random_start = False\n",
    "\n",
    "attack = CFoolboxPGDLinf(classifier=clf, \n",
    "                        epsilons=epsilon,\n",
    "                        abs_stepsize=abs_stepsize,\n",
    "                        steps=steps,\n",
    "                        random_start=False)\n",
    "\n",
    "\n",
    "# we only need the predictions to compute the robust accuracy\n",
    "y_pred_adv, _, _, _ = attack.run(samples, labels)\n",
    "\n",
    "accuracy = metric.performance_score(y_true=labels, y_pred=y_pred)\n",
    "robust_accuracy = metric.performance_score(y_true=labels, y_pred=y_pred_adv)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Robust accuracy: {robust_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Did it work?\n",
    "Probably not as expected... But the model is not as robust as it seems.\n",
    "Let's try to visualize the loss of the attack on one single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "point = 0\n",
    "x0, y0 = ts.X[point, :], ts.Y[point]\n",
    "pred = clf.predict(x0)\n",
    "y_pred_adv, _, _, _ = attack.run(x0, y0)\n",
    "\n",
    "print(f\"True label: {y0}\\nOriginal pred.: {pred}\\nAdv. pred.: {y_pred_adv}\")\n",
    "\n",
    "from secml.figure import CFigure\n",
    "fig = CFigure()\n",
    "\n",
    "# this stores the path of the attack\n",
    "path = attack.x_seq\n",
    "fig.sp.plot(attack.objective_function(path))\n",
    "fig.sp.xlabel(\"steps\")\n",
    "fig.sp.ylabel(\"loss\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The attack is not working because the optimization is not really improving the objective.\n",
    "This is caused by gradient obfuscation.\n",
    "As explained before, the model is indeed causing the loss landscape to become extremely noisy (this is shown also in the paper).\n",
    "\n",
    "![](assets/kWTA_landscape.png)\n",
    "\n",
    "How to fix this? We need an **adaptive attack**[2].\n",
    "\n",
    "[2] Tramer, Florian, et al. \"On adaptive attacks to adversarial example defenses.\" Advances in Neural Information Processing Systems 33 (2020): 1633-1645.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Open the referenced paper [2] and find the model that we just attacked and read through the section (hint: go to https://arxiv.org and find the paper there).\n",
    "What causes the model to be strong against gradient-based attacks?\n",
    "How did the authors of [2] break the defense?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Adaptive attack for k-WTA\n",
    "\n",
    "We are now using an implementation of the attack in [2] (adapted for SecML).\n",
    "\n",
    "The attack estimates the gradient by querying the model (without computing the white-box gradient), and by computing a finite-difference approximation on sets of points sampled in a neighborhood of the sample.\n",
    "The smoother approximation is then obtained by averaging the direction of all these estimated gradients.\n",
    "\n",
    "This approximation has a cost: it has to query the model multiple time for each step of the attack, as the gradient has to be estimated locally each time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from attacks.averaged_pgd import CFoolboxAveragedPGD\n",
    "\n",
    "# note: it takes longer to run this attack than regular PGD!\n",
    "smooth_attack = CFoolboxAveragedPGD(classifier=clf, epsilons=epsilon, abs_stepsize=abs_stepsize, steps=steps, random_start=random_start, k=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "y_pred_adv_smooth, _, _, _ = smooth_attack.run(x0, y0)\n",
    "\n",
    "print(f\"True label: {y0}\\nOriginal pred.: {pred}\\nAdv. pred.: {y_pred_adv_smooth}\")\n",
    "\n",
    "fig = CFigure()\n",
    "\n",
    "# this stores the path of the attack\n",
    "path = smooth_attack.x_seq\n",
    "fig.sp.plot(smooth_attack.objective_function(path))\n",
    "fig.sp.xlabel(\"steps\")\n",
    "fig.sp.ylabel(\"loss\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now the loss is smoother. One can potentially make it even smoother by changing one parameter of the attack. Which one?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Defensive distillation\n",
    "\n",
    "The defensive distillation method first trains an initial network $f$ on data $x$ with a softmax temperature of $T$. Then it uses the probability vector $f(x)$, which includes additional knowledge about classes compared to a class label, predicted by network $f$, to train a distilled network $f_d$ at temperature $T$ on the same data $x$.\n",
    "\n",
    "In other words, the network $f_d$ is trained using *soft labels*. In fact, they might not even be correct, but they are representation of what the model $f$ has learnt.\n",
    "\n",
    "![](assets/distillation.png)\n",
    "\n",
    "[3] Papernot, Nicolas, et al. \"Distillation as a defense to adversarial perturbations against deep neural networks.\" 2016 IEEE symposium on security and privacy (SP). IEEE, 2016.\n",
    "\n",
    "The idea behind this defense is that the student network should learn a \"hardened\" function that is constant around the tested samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from models.distillation import MNIST9Layer\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from robustbench.utils import download_gdrive\n",
    "from secml.array import CArray\n",
    "from secml.ml import CClassifierPyTorch\n",
    "from torch import nn\n",
    "\n",
    "MODEL_ID = '1s7Kfa2Bs5nY2zLd6dVAxUqNbCNQhPYxs'\n",
    "\n",
    "model = MNIST9Layer()\n",
    "path = os.path.join('pretrained', 'mnist_distilled.pt')\n",
    "if not os.path.exists(path):\n",
    "    download_gdrive(MODEL_ID, path)\n",
    "state_dict = torch.load(path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "clf = CClassifierPyTorch(model, input_shape=(1, 28, 28), pretrained=True,\n",
    "                         pretrained_classes=CArray(list(range(10))), preprocess=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load MNIST dataset (we only need the test set)\n",
    "from secml.data.loader import CDataLoaderMNIST\n",
    "ts = CDataLoaderMNIST().load('testing')\n",
    "\n",
    "# let's use a subset of the data\n",
    "ts = ts[:100, :]\n",
    "\n",
    "# transform data to stay in [0, 1] bounds\n",
    "ts.X /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Test the classification accuracy of the distilled network, and attack it again with PGD. Remember that we have to define again the attack for every new classifier. The attack hyperparameters will also be different.\n",
    "Use the following hyperparameters:\n",
    "* $\\varepsilon=0.3$\n",
    "* $\\alpha=0.05$\n",
    "* steps = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = 100\n",
    "samples, labels = ts.X[:idx, :], ts.Y[:idx]\n",
    "\n",
    "y_pred = clf.predict(samples)\n",
    "\n",
    "epsilon = 0.3\n",
    "abs_stepsize = 0.05\n",
    "steps = 100\n",
    "random_start = False\n",
    "\n",
    "from secml.adv.attacks import CFoolboxPGDLinf\n",
    "attack = CFoolboxPGDLinf(classifier=clf, \n",
    "                        epsilons=epsilon, abs_stepsize=abs_stepsize,\n",
    "                        steps=steps, random_start=random_start)\n",
    "\n",
    "from secml.ml.peval.metrics import CMetricAccuracy\n",
    "metric = CMetricAccuracy()\n",
    "\n",
    "# we only need the prediction to compute the robust accuracy\n",
    "y_pred_adv, _, _, _ = attack.run(samples, labels)\n",
    "\n",
    "accuracy = metric.performance_score(y_true=labels, y_pred=y_pred)\n",
    "robust_accuracy = metric.performance_score(y_true=labels, y_pred=y_pred_adv)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Robust accuracy: {robust_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The robust accuracy after the attack is almost the same as the original accuracy.\n",
    "Let's check again the loss function to understand the problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "point = 0\n",
    "x0, y0 = ts.X[point, :], ts.Y[point]\n",
    "pred = clf.predict(x0)\n",
    "y_pred_adv, _, _, _ = attack.run(x0, y0)\n",
    "\n",
    "print(f\"True label: {y0}\\nOriginal pred.: {pred}\\nAdv. pred.: {y_pred_adv}\")\n",
    "\n",
    "from secml.figure import CFigure\n",
    "fig = CFigure()\n",
    "\n",
    "# this stores the path of the attack\n",
    "path = attack.x_seq\n",
    "fig.sp.plot(attack.objective_function(path))\n",
    "fig.sp.xlabel(\"steps\")\n",
    "fig.sp.ylabel(\"loss\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As the gradient update is defined as:\n",
    "\n",
    "$x_{i+1} = x_i + \\alpha \\nabla f(x_i, y, \\theta)$,\n",
    "\n",
    "there might be two possibilities. Either the step size is too small, or the gradient is zero!\n",
    "\n",
    "If the gradient is zero, the point $x_i$ is never updated!\n",
    "Let's check the size of the gradient (any norm is fine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient = attack.objective_function_gradient(x0)\n",
    "\n",
    "gradient_size = gradient.norm()\n",
    "\n",
    "print(\"L2 norm of the gradient:\", gradient_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The distillation defense leverages a specific trick that makes the softmax function saturate, hence making its computations on the gradient *unstable*.\n",
    "\n",
    "The softmax function is usually cascaded after DNNs to get outputs that look like probabilities, *i.e.*, to sum up to 1.\n",
    "\n",
    "This defense can be broken by removing the softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from attacks.logits_pgd import CFoolboxLogitsPGD\n",
    "\n",
    "logits_attack = CFoolboxLogitsPGD(classifier=clf, epsilons=epsilon, abs_stepsize=abs_stepsize, steps=steps,\n",
    "                                    random_start=random_start)\n",
    "y_pred_adv_logits, _, _, _ = logits_attack.run(x0, y0)\n",
    "\n",
    "print(f\"True label: {y0}\\nOriginal pred.: {pred}\\nAdv. pred.: {y_pred_adv_logits}\")\n",
    "\n",
    "fig = CFigure()\n",
    "\n",
    "# this stores the path of the attack\n",
    "path = logits_attack.x_seq\n",
    "fig.sp.plot(logits_attack.objective_function(path))\n",
    "fig.sp.xlabel(\"steps\")\n",
    "fig.sp.ylabel(\"loss\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Compute the gradient of the new attack function and show its norm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gradient = logits_attack.objective_function_gradient(x0)\n",
    "\n",
    "gradient_size = gradient.norm()\n",
    "\n",
    "print(\"L2 norm of the gradient:\", gradient_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise\n",
    "\n",
    "Another strategy might have worked in this case.\n",
    "Hint: remember that the distilled network is learning an approximation of the teacher network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MODEL_ID = '1qmjepc4k_o4BIqCvUJgwg-K19CaovOrQ'\n",
    "\n",
    "teacher_model = MNIST9Layer()\n",
    "teacher_path = os.path.join('pretrained', 'mnist_teacher.pt')\n",
    "if not os.path.exists(teacher_path):\n",
    "    download_gdrive(MODEL_ID, teacher_path)\n",
    "teacher_state_dict = torch.load(teacher_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "teacher_model.load_state_dict(teacher_state_dict)\n",
    "teacher_model.eval()\n",
    "\n",
    "teacher_clf = CClassifierPyTorch(teacher_model, input_shape=(1, 28, 28), pretrained=True,\n",
    "                                 pretrained_classes=CArray(list(range(10))), preprocess=None)\n",
    "\n",
    "\n",
    "\n",
    "idx = 10\n",
    "samples, labels = ts.X[:idx, :], ts.Y[:idx]\n",
    "\n",
    "epsilon = 0.3\n",
    "abs_stepsize = 0.05\n",
    "steps = 100\n",
    "random_start = False\n",
    "attack_teacher = CFoolboxPGDLinf(classifier=teacher_clf,\n",
    "                                 epsilons=epsilon, abs_stepsize=abs_stepsize,\n",
    "                                 steps=steps, random_start=random_start)\n",
    "\n",
    "# we only need the prediction to compute the robust accuracy\n",
    "y_pred_adv_teacher, _, adv_ds, _ = attack_teacher.run(samples, labels)\n",
    "\n",
    "y_pred_student = clf.predict(samples)\n",
    "y_pred_adv_student = clf.predict(adv_ds.X)\n",
    "\n",
    "accuracy = metric.performance_score(y_true=labels, y_pred=y_pred_student)\n",
    "robust_accuracy_teacher = metric.performance_score(y_true=labels, y_pred=y_pred_adv_teacher)\n",
    "robust_accuracy_student = metric.performance_score(y_true=labels, y_pred=y_pred_adv_student)\n",
    "\n",
    "print(f\"Robust accuracy teacher: {robust_accuracy_teacher}\")\n",
    "\n",
    "print(f\"Accuracy student: {accuracy}\")\n",
    "print(f\"Robust accuracy student: {robust_accuracy_student}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "point = 0\n",
    "x0, y0 = ts.X[point, :], ts.Y[point]\n",
    "pred = teacher_clf.predict(x0)\n",
    "y_pred_adv, _, _, _ = attack_teacher.run(x0, y0)\n",
    "\n",
    "print(f\"True label: {y0}\\nOriginal pred.: {pred}\\nAdv. pred.: {y_pred_adv}\")\n",
    "\n",
    "fig = CFigure()\n",
    "\n",
    "# this stores the path of the attack\n",
    "path = attack_teacher.x_seq\n",
    "fig.sp.plot(attack_teacher.objective_function(path))\n",
    "fig.sp.xlabel(\"steps\")\n",
    "fig.sp.ylabel(\"loss\")\n",
    "fig.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('secml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6829dbcfe73f7e6ba320fd39e7c4bddd23e92d1a15475ecfb0305a1647487c5f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
